# -*- coding: utf-8 -*-
"""ASD_DCGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ueRKpg9wlvEBsnHcBgCr3KhO26jzqvDL
"""

import pandas as pd
import numpy as np
import sys
import inspect
import sklearn
from sklearn import cluster, preprocessing
from sklearn.preprocessing import normalize, scale, MinMaxScaler, LabelBinarizer
from sklearn.metrics import mean_squared_error, confusion_matrix, accuracy_score, roc_auc_score,roc_curve, precision_recall_curve,auc, f1_score,silhouette_score,normalized_mutual_info_score, adjusted_rand_score
from sklearn.model_selection import train_test_split 
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn import linear_model
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
from torch import reshape,cat,multiply, flatten
from torch.nn import init,Sequential, Module
from torch.nn import Linear, BatchNorm1d,BatchNorm2d,BatchNorm3d, Dropout, Dropout2d, Flatten, ZeroPad2d, ConvTranspose2d, ConvTranspose3d
from torch.nn import LeakyReLU, ReLU, Softmax, Tanh, Sigmoid
from torch.nn import UpsamplingBilinear2d, Upsample,Conv2d,MaxPool2d,AvgPool1d,AvgPool2d,Conv1d,AdaptiveAvgPool1d,AdaptiveMaxPool1d, MaxPool1d,UpsamplingNearest2d,MaxPool3d
from torch.nn import MSELoss,L1Loss,CrossEntropyLoss,SoftMarginLoss,BCELoss,BCEWithLogitsLoss,HingeEmbeddingLoss
from torch.autograd import Variable
from torch.optim import Adamax, Adam
from torch.utils.data import DataLoader, TensorDataset
from typing import Tuple
from torch.nn.modules.activation import Softmax2d

import datetime
from datetime import timedelta



import tensorflow as tf

from tf_keras import utils

from tensorflow.python.client import device_lib

import scipy
from scipy.io import wavfile
from scipy.signal import butter,lfilter,filtfilt,lfilter_zi,sosfilt
import pickle
import random
from scipy.spatial.distance import sqeuclidean
import os
import math

device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
dtype=torch.int64
sys.setrecursionlimit(10000)
print(torch.cuda.device_count())
gpu_unit = torch.cuda.device_count()
learning_rate_d=0.0002
learning_rate_g=0.0002
learning_rate_c=0.0004
criterion=torch.nn.MSELoss()
#criterion=torch.nn.BCELoss()
beta1=0.5
batch_size=16
epochs=100
class BuildGenerator(torch.nn.Module):
    
    def __init__(self,
                 gpu_unit):
        super(BuildGenerator,self).__init__()
        

        self.gpu_unit=gpu_unit

        
        # Build the generator.
        self.generatorLayer_1=Sequential(
                ConvTranspose2d(1,64,kernel_size=1,stride=12,padding=1),
                BatchNorm2d(num_features=64,momentum=0.8),
                LeakyReLU(0.2),
                Dropout2d(0.1))
        self.generatorLayer_2=Sequential(
                ConvTranspose2d(64,128,kernel_size=1,stride=1,padding=0),
                BatchNorm2d(num_features=128,momentum=0.8),
                LeakyReLU(0.2),
                Dropout2d(0.1))

        self.generatorLayer_3=Sequential(
                ConvTranspose2d(128,256,kernel_size=1,stride=1,padding=0),
                BatchNorm2d(num_features=256,momentum=0.8),
                LeakyReLU(0.2),
                Dropout2d(0.1))

        self.generatorLayer_4=Sequential(
                ConvTranspose2d(256,1,kernel_size=1,stride=3,padding=0),
                BatchNorm2d(num_features=1,momentum=0.8),
                LeakyReLU(0.2),
                Softmax2d(),
                Dropout2d(0.2))
              

    def forward(self,input):
        print('generator input: ',input.shape)
        main_g=self.generatorLayer_1(input)
        print("generator layer 1: ", main_g.shape)
        main_g=self.generatorLayer_2(main_g)
        print("generator layer 2: ", main_g.shape)
        main_g=self.generatorLayer_3(main_g)
        print("generator layer 3: ", main_g.shape)
        out=self.generatorLayer_4(main_g)
        print("generator layer 4: ", out.shape)
        return out

        
class BuildDiscriminator(torch.nn.Module):
    def __init__(self,
                 gpu_unit,
                 latency_dim,
                 features,
                 num_classes):
        super(BuildDiscriminator,self).__init__()

        self.gpu_unit=gpu_unit
        self.latency_dim=latency_dim
        self.num_classes=num_classes
        self.features=features

        
        # Build the discriminator.
        self.discriminatorLayer_1=Sequential(Conv2d(1,64,kernel_size=1,stride=1,padding='valid'),
                         BatchNorm2d(num_features=64,momentum=0.8),
                         LeakyReLU(0.2),
                         Dropout2d(0.1))
        self.discriminatorLayer_2=Sequential(Conv2d(64,128,kernel_size=1,stride=1,padding='valid'),
                         BatchNorm2d(num_features=128,momentum=0.8),
                         LeakyReLU(0.2),
                         Dropout2d(0.1))
      
        self.discriminatorLayer_3=Sequential(Conv2d(128,256,kernel_size=1,stride=1,padding='valid'),
                         BatchNorm2d(num_features=256,momentum=0.8),
                         LeakyReLU(0.2),
                         Dropout2d(0.1))    
        
        self.discriminatorLayer_4=Sequential(Conv2d(256,1,kernel_size=1,stride=1,padding="same"),
                         BatchNorm2d(num_features=1,momentum=0.8),
                         LeakyReLU(0.2),
                         Dropout2d(0.1))
        self.fc_1=Linear(self.features,self.features,bias=True)
        torch.nn.init.xavier_uniform_(self.fc_1.weight)
        self.fcLayer_1=Sequential(self.fc_1,ReLU(True),Dropout2d(0.2))
        self.fc_2=Linear(self.features,self.num_classes,bias=True)
        torch.nn.init.xavier_uniform_(self.fc_2.weight)
        self.fcLayer_2=Sequential(self.fc_2,Sigmoid())

    def forward(self,input):
        print('input of discriminator: ',input.shape)
        main_d=self.discriminatorLayer_1(input)
        print("discriminator layer 1: ", main_d.shape)
        main_d=self.discriminatorLayer_2(main_d)
        print("discriminator layer 2: ", main_d.shape)
        main_d=self.discriminatorLayer_3(main_d)
        print("discriminator layer 3: ", main_d.shape)
        main_d=self.discriminatorLayer_4(main_d)
        print("discriminator layer 4: ", main_d.shape)
        fc1=self.fcLayer_1(main_d)
        print(fc1.shape)
        fl=flatten(fc1, start_dim=1,end_dim=2)
        print(fl.shape)
        out=self.fcLayer_2(fl)
        print(out.shape)
        return out

class DCGANAnalysis():
    
    def weights_init(m):
        classname=m.__class__.__name__
        if classname.find('Conv')!=-1:
            nn.init.normal_(m.weight.data,0.0,0.02)
        elif classname.find('BatchNorm')!=-1:
            nn.init.normal_(m.weight.data,1,0.02)
            nn.init.constant_(m.bias.data,0)
            
    def __init__(self,
                 latency_dim=None,
                 epochs=None,
                 gpu_unit=None,
                 features=None,
                 num_classes=None):
        args, _, _, values = inspect.getargvalues(inspect.currentframe())
        values.pop("self")
        


        
        for arg, val in values.items():
            setattr(self, arg, val)
        
        global optimizer_c,optimizer_d,optimizer_g
        

        self.latency_dim=latency_dim
        self.epochs=epochs
        self.gpu_unit=gpu_unit
        self.num_classes=num_classes
        self.features=features
        
        self.generator=BuildGenerator(self.gpu_unit).to(device)
        if (device.type=='cuda') and (self.gpu_unit>1):
            self.generator=nn.DataParallel(self.generator,list(range(self.gpu_unit)))
            
        #self.generator.apply(DCGANAnalysis("").weights_init)
        #print(self.generator)
        optimizer_g = Adam(self.generator.parameters(),lr=learning_rate_g, betas=(beta1,0.999))
        
   

        
        self.discriminator=BuildDiscriminator(self.gpu_unit,self.latency_dim,self.features,self.num_classes).to(device)
        #print(self.discriminator)      
        if (device.type=='cuda') and (self.gpu_unit>1):
            self.discriminator=nn.DataParallel(self.discriminator,list(range(self.gpu_unit)))
            
        #self.discriminator.apply(DCGANAnalysis("").weights_init)                

        optimizer_d = Adam(self.discriminator.parameters(),lr=learning_rate_d, betas=(beta1,0.999))
        optimizer_c = Adam(self.discriminator.parameters(self.generator.parameters()),lr=learning_rate_d, betas=(beta1,0.999))



 
    def fit(self,            
            X,
            epochs=None):
        
        global scaler
        
        num_train = X.shape[0]
        start = 0
        
        
        
        # Adversarial ground truths.
        #noise_=torch.randn(X.size(0),1,X.size(1),2)

          
        
        
        #scaler=MinMaxScaler()
        generator_losses=[]
        discriminator_losses=[]

        for epoch in range(self.epochs):

            self.discriminator.zero_grad()

            idx=np.random.randint(low=0,high=X.shape[0],size=batch_size)
            raw_data=X[idx]
            #print(raw_data.shape)
            #print(raw_data.size(0))


            # Get a batch of real returns data...

            stop = start + batch_size
            raw_data = X[start:stop].to(device)
            #print('shape of raw data',raw_data.shape)
            output_real=self.discriminator(raw_data)
            #print('shape of output real data',output_real.shape)
            valid=torch.full(size=(output_real.size(0),output_real.size(1),output_real.size(2)),fill_value=1,dtype=torch.float,device=device)
            #print('shape of valid', valid.shape)
            d_loss_real = criterion(output_real, valid)                
            d_loss_real.backward()
            D_x=output_real.mean().item()

            # Train the discriminator.
            optimizer_d.step()
            # Generate a new batch of noise...
            noise=torch.randn(raw_data.size(0),raw_data.size(1),raw_data.size(2),raw_data.size(3),device=device)
            #print('the shape of noise is:',noise.shape)

            # ...and generate a batch of synthetic returns data.

            generated_data = self.generator(noise)
            output_fake=self.discriminator(generated_data.detach())
            fake=torch.full(size=(output_fake.size(0),output_fake.size(1),output_fake.size(2)),fill_value=0,dtype=torch.float,device=device)

            d_loss_fake = criterion(output_fake,fake)
            d_loss_fake.backward()
            D_G_z1=output_fake.mean().item()
            d_loss = 0.5 * torch.add(d_loss_real, d_loss_fake)
            

            
          
            self.generator.zero_grad()    
            output=self.discriminator(generated_data)
            g_loss = criterion(output, valid)
            g_loss.backward()
            D_G_z2=output.mean().item()
            
            optimizer_g.step()
            
            optimizer_c.step()


            
            start +=batch_size
            

            if start>num_train-batch_size:
                start=0

            
            if epoch %20==0:
                print("Epoch={},\t Loss_D={:2.4f},\t Loss_G={},,\t D_x={},\t D(G(z))_1={:2.4f},\t D(G(z))_2={}".format(epoch+1,d_loss.item(),g_loss.item(),D_x,D_G_z1,D_G_z2))

         
            
            generator_losses.append(g_loss.item())
            discriminator_losses.append(d_loss.item())
            
            epoch +=1
 
        return output



class myDCGAN():
       
    def train(x_train,y_train,x_test,y_test, num_classes):

        global features
        x_train = np.array(x_train)
        x_test = np.array(x_test)
        features=x_train.shape[1]
        print("number of feature is ",features)
        latency_dim=batch_size


        x_train = np.reshape(x_train,(x_train.shape[0],1,1,x_train.shape[1]))
        x_test = np.reshape(x_test,(x_test.shape[0],1,1,x_test.shape[1]))
        #print(len(df_new_))

        #print(x_train.shape[0])
        y_train = np.reshape(y_train,(len(y_train),1,1))
        y_train=utils.to_categorical(y_train,num_classes)
        #print(y_train)
        #print(y_train.shape)

        y_test = np.reshape(y_test,(len(y_test),1,1))
        y_test =utils.to_categorical(y_test,num_classes)
   
        dataset_train=TensorDataset(torch.Tensor(x_train),torch.Tensor(y_train))
           
        dataset_test=TensorDataset(torch.Tensor(x_test),torch.Tensor(y_test))  

        data_train=torch.utils.data.DataLoader(dataset=dataset_train,batch_size=batch_size,shuffle=True)
        data_test=torch.utils.data.DataLoader(dataset=dataset_test,batch_size=batch_size,shuffle=True)
        


        y_train_predict=[]
        for i, (batch_X,batch_Y) in enumerate (data_train):
                X=Variable(batch_X)
                Y=Variable(batch_Y)
                
                X,Y=X.to(device),Y.to(device)      
                dcgan = DCGANAnalysis(
                      latency_dim=latency_dim,
                      epochs=epochs,
                      gpu_unit=gpu_unit,
                      features=features,
                      num_classes=num_classes)
        
                y_train_predict_=dcgan.fit(X=X,epochs=epochs)
                y_train_predict_ = y_train_predict_[:,0:1,:]
                y_train_predict.append(y_train_predict_.detach().cpu().numpy())
                
                i+=1
        
        y_train_predict=np.concatenate(y_train_predict,axis=0)
        #print(y_train_predict)
        y_train_predict=np.reshape(y_train_predict,(int(y_train_predict.shape[0]*y_train_predict.shape[1]),y_train_predict.shape[2]))
        y_train=np.reshape(y_train,(int(y_train.shape[0]*y_train.shape[1]),y_train.shape[2]))
        y_test_predict=[]
        for j, (batch_x_test,batch_y_test) in enumerate (data_test):
                X_=Variable(batch_x_test)   
                X_=X_.to(device)
          
                dcgan = DCGANAnalysis(
                      latency_dim=latency_dim,
                      epochs=epochs,
                      gpu_unit=gpu_unit,
                      features=features,
                      num_classes=num_classes)
        
                y_test_predict_=dcgan.fit(X=X_,epochs=epochs)
                y_test_predict_ = y_test_predict_[:,0:1,:]
                #y_predict_= reshape(y_predict_,(y_predict_.shape[0],y_predict_.shape[2],y_predict_.shape[1]))
                y_test_predict.append(y_test_predict_.detach().cpu().numpy())
                j+=1
      



        
        y_test_predict=np.concatenate(y_test_predict,axis=0)

        y_test_predict=np.reshape(y_test_predict,(int(y_test_predict.shape[0]*y_test_predict.shape[1]),y_test_predict.shape[2]))
        y_test=np.reshape(y_test,(int(y_test.shape[0]*y_test.shape[1]),y_test.shape[2]))

        y_predict=[]
        for i in range(len(y_test_predict)):
            print(len(y_test_predict[i]))
            y_predict.append(np.argmax(y_test_predict[i]))
        '''
        y_predict=[]
        for i in range(len(y_test_predict)):
            #print(y_test_predict.shape)
            #print(y_test_predict[i])
            max_value = max(y_test_predict[i])
            #print(max_value)

            if max_value == 0:
                #y_predict.append(4)
                relationships = x_test_orig[i]
                #print(relationships)                
                selected_classification = similarityAlgo.similarity(relationships,classes)
                print(i,selected_classification)
                y_predict.append(selected_classification )  
            else:
                for j in range(len(y_test_predict[i])):
                    if y_test_predict[i][j] == max_value:
                        print(i,j,max_value)
                        y_predict.append(j)  '
        '''
        #testScore=math.sqrt(mean_squared_error(y_test,y_predict)) 
        #print('Test Score: %.5f RMSE' % (testScore))
        model_name = 'DCGAN'
        return y_predict, model_name
